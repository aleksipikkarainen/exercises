{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = red> Please fill up the asked information!\n",
    "    \n",
    "<font color = teal>Name: Aleksi Pikkarainen\n",
    "\n",
    "<font color = teal>Student number: ******\n",
    "\n",
    "<font color = teal>Mail: akpikk@utu.fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rpio-nMAoDwq"
   },
   "source": [
    "------\n",
    "\n",
    "# Data Analysis and Knowledge Discovery: Exercise 2, Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90c9fF6woDwr"
   },
   "source": [
    "The previous exercise was about <i>data understanding</i> and <i>data preparation</i>, which formed the basis for the modeling phase of the data mining process. Many modeling techniques make assumptions about the data, so the exploration and preparation phases can't be ignored. Now, as we have checked the validity of data and familiarized ourselves with it, we can move on to the next stage of the Cross-Industry Standard Process for Data Mining (CRISP-DM), which is <font color = green>modeling</font>.\n",
    "\n",
    "The questions to be answered at this stage could include:\n",
    "\n",
    "- What kind of model architecture best fits our data?\n",
    "- How well does the model perform technically?\n",
    "- Could we improve its performance?\n",
    "- How do we evaluate the model's performance?\n",
    "\n",
    "<i>Machine learning</i> is a subfield of artificial intelligence that provides automatic, objective and data-driven techniques for modeling the data. Its two main branches are <i>supervised learning</i> and <i>unsupervised learning</i>, and in this exercise, we are going to use the former, <font color = green>supervised learning</font>, for classification and regression tasks.\n",
    "\n",
    "For classification, data remains the same as in the previous exercise, but I've already cleaned it up for you. Some data pre-processing steps are still required to ensure that it's in an appropriate format so that models can learn from it. Even though we are not conducting any major data exploration nor data preparation this time, <i>you should <b>never</b> forget it in your future data analyses</i>.\n",
    "\n",
    "-----\n",
    "\n",
    "<b>General guidance for exercises</b>\n",
    "\n",
    "- Answer <b>all</b> questions below, even if you can't get your script to fully work.\n",
    "- Write clear and easily readable code, and include explanations what your code does.\n",
    "- Make informative illustrations: include labels for x and y axes, legends and captions for your plots.\n",
    "- You can add more code and markup cells, as long as the flow of the notebook stays readable and logical.\n",
    "- <b>A complete submission includes a working notebook</b>, so it's highly recommended to run \"Restart & Run all\" before the final save. Remember to submit BOTH versions of the exercise (ipynb AND html/pdf). \n",
    "- Grading: *Fail*/*Pass*/*Pass with honors* (+1)\n",
    "    - Passing requires that <b>the parts 1-5</b> are completed.\n",
    "    - +1 bonus point (grading *Pass with honors*) requires a <b>completely</b> correct solution and also thorough analysis.\n",
    "- If you encounter problems, Google first. Always give the credit where it belongs so please <b>cite your sources, whether you're referencing text or code</b>. You will learn so much more when you have to research and summarize information in your own words. If you can't find an answer to the problem, don't hesitate to ask in the Moodle discussion or directly via email (tuhlei@utu.fi).\n",
    "- Note! Don't leave it to the last moment! No feedback service during weekends.\n",
    "- <b>We do not encourage the use of ChatGPT or similar models</b>, but if you choose to do so, always be critical of the outputs and try to comprehend them before any use. Also, make a brief description how you utilized the model (what was your input and how did you benefit from the output).\n",
    "\n",
    "\n",
    "<font color = green> The guided exercise session is held on the 28th of November at 14:15-16:00, at TSE Elovena-Sali.</font>\n",
    "\n",
    "<font color = red size = 4><b>The deadline is the 30th of November at 23:59</b></font>. Late submissions will not be accepted unless there is a valid excuse for an extension which should be asked **before** the original deadline.\n",
    "\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2I2WLapM3BPc"
   },
   "source": [
    "### Gather all packages needed for this notebook here:\n",
    "\n",
    "You can use other packages as well, but this excercise can be completed with those below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iypIAVquoDws"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization packages - matplotlib and seaborn\n",
    "# Remember that pandas is also handy and capable when it comes to plotting!\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine learning package - scikit-learn\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneOut, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "\n",
    "# Show the plots inline in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________\n",
    "## <font color = lightcoral>1. Classification using k-nearest neighbors </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CqWZYx2oDw3"
   },
   "source": [
    "We start exploring the world of data modeling by using the <font color = lightcoral>K-Nearest Neightbors (k-NN) algorithm</font>. The k-NN algorithm is one of the classic supervised machine learning algorithms which assumes that similar points are close to each other. \n",
    "\n",
    "In our case, we'll use the k-NN algorithm to *predict the presence of cardiovascular disease* (CVD) using all the other variables as <font color = lightcoral>features</font> in the given data set. I.e. the <font color = lightcoral>target variable</font> that we are interested in is `cardio`.\n",
    "\n",
    "But first, we need data for the task. The code for loading the data into the environment is provided for you. The code should work but make sure that you have the CSV file of the data in the same directory where you have this notebook file.\n",
    "\n",
    "**Exercise 1 A)**\n",
    "\n",
    "Take a random sample of 1500 rows from the dataframe using your student id as a seed. Print the first 15 rows to check that everything is ok with the dataframe.\n",
    "\n",
    "*Note: as mentioned, the data remains the same, but cholesterol has been one-hot-encoded for you already. There's also a new variable, gluc (about glucose levels), which is one-hot-encoded for you. It has similar values as cholesterol originally had [normal, at risk, elevated].*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading code provided\n",
    "# ------------------------------------------------------\n",
    "# The data file should be at the same location than the \n",
    "# exercise file to make sure the following lines work!\n",
    "# Otherwise, fix the path.\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# Path for the data\n",
    "data_path = 'ex2_cardio_data.csv'\n",
    "\n",
    "# Read the CSV file \n",
    "cardio_data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "      <th>cholesterol_normal</th>\n",
       "      <th>cholesterol_elevated</th>\n",
       "      <th>cholesterol_high</th>\n",
       "      <th>gluc_normal</th>\n",
       "      <th>gluc_elevated</th>\n",
       "      <th>gluc_high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3850</th>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>95.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>159</td>\n",
       "      <td>55.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>90.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>164</td>\n",
       "      <td>67.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3651</th>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>165</td>\n",
       "      <td>66.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2840</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>68.0</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>165</td>\n",
       "      <td>86.0</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2814</th>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>72.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4973</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>65.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "      <td>56.0</td>\n",
       "      <td>103</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2728</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>65.0</td>\n",
       "      <td>100</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4617</th>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>63.0</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833</th>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>73.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4217</th>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>75.0</td>\n",
       "      <td>150</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5134</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>68.0</td>\n",
       "      <td>190</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  gender  height  weight  ap_hi  ap_lo  smoke  alco  active  cardio  \\\n",
       "3850   43       2     172    95.0    110     80      0     0       1       0   \n",
       "17     54       1     159    55.0    110     80      0     0       0       0   \n",
       "2031   52       1     168    90.0    120     80      0     0       0       0   \n",
       "1368   39       1     164    67.0    140     90      1     0       1       0   \n",
       "3651   52       2     165    66.0    130     70      0     0       0       0   \n",
       "2840   49       1     162    68.0    130     80      0     0       1       0   \n",
       "4400   51       2     165    86.0    130     80      0     0       1       1   \n",
       "2814   43       1     168    72.0    120     80      0     0       1       0   \n",
       "4973   57       1     162    65.0    120     80      0     0       1       1   \n",
       "5      53       1     152    56.0    103     65      0     0       1       0   \n",
       "2728   47       1     161    65.0    100     70      0     0       1       0   \n",
       "4617   55       2     160    63.0    130     90      1     0       1       1   \n",
       "1833   44       2     169    73.0    120     80      0     0       1       0   \n",
       "4217   57       2     168    75.0    150     90      0     0       1       1   \n",
       "5134   53       1     158    68.0    190    100      0     0       1       1   \n",
       "\n",
       "      cholesterol_normal  cholesterol_elevated  cholesterol_high  gluc_normal  \\\n",
       "3850                   1                     0                 0            1   \n",
       "17                     1                     0                 0            1   \n",
       "2031                   0                     1                 0            1   \n",
       "1368                   1                     0                 0            1   \n",
       "3651                   1                     0                 0            1   \n",
       "2840                   1                     0                 0            1   \n",
       "4400                   0                     1                 0            0   \n",
       "2814                   1                     0                 0            1   \n",
       "4973                   1                     0                 0            1   \n",
       "5                      1                     0                 0            1   \n",
       "2728                   1                     0                 0            1   \n",
       "4617                   1                     0                 0            1   \n",
       "1833                   0                     1                 0            1   \n",
       "4217                   1                     0                 0            1   \n",
       "5134                   1                     0                 0            1   \n",
       "\n",
       "      gluc_elevated  gluc_high  \n",
       "3850              0          0  \n",
       "17                0          0  \n",
       "2031              0          0  \n",
       "1368              0          0  \n",
       "3651              0          0  \n",
       "2840              0          0  \n",
       "4400              1          0  \n",
       "2814              0          0  \n",
       "4973              0          0  \n",
       "5                 0          0  \n",
       "2728              0          0  \n",
       "4617              0          0  \n",
       "1833              0          0  \n",
       "4217              0          0  \n",
       "5134              0          0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Code - Resample and print 15 rows\n",
    "\n",
    "#Let's take a random sample of 1500 rows from the data first.\n",
    "sampled_data = cardio_data.sample(n=1500, random_state=519153)\n",
    "\n",
    "#Then we'll print the first 15 rows.\n",
    "sampled_data.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfq_3_WNoDw3"
   },
   "source": [
    "----\n",
    "\n",
    "We have the data so now, let's put it to use. \n",
    "\n",
    "To teach the k-NN algorithm (or any other machine learning algorithm) to recognize patterns, we need <font color = lightcoral>training data</font>. However, to assess how well a model has learned these patterns, we require <font color = lightcoral>test data</font> which is new and unseen by the trained model. It's important to note that the test set is not revealed to the model until after the training is complete.\n",
    "\n",
    "So, to *estimate the performance of a model*, we may use a basic <font color = lightcoral>train-test split</font>. The term \"split\" is there because we literally split the data into two sets.\n",
    "\n",
    "**Exercise 1 B)**\n",
    "\n",
    "Collect the features as an array named `features`, and the target variable as an array named `labels`. Create training and test data by randomly splitting the data into training (80%) and test (20%) sets.\n",
    "\n",
    "- Do you need stratification for our dataset? Explain your decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - Train-test split\n",
    "\n",
    "#Let's collect the features into an array. Since we want the \"cardio\" as our target, we can include everything else here\n",
    "features = ['age', 'gender', 'height', 'weight', 'weight', 'ap_hi', 'ap_lo', 'smoke', 'alco', 'active','cholesterol_normal', 'cholesterol_elevated', 'cholesterol_high', 'gluc_normal', 'gluc_elevated', 'gluc_high' ]\n",
    "\n",
    "#Let's collect the target variable into an array.\n",
    "labels = ['cardio']\n",
    "\n",
    "#Let's split the data randomly into training and test sets with a 80/20 split.\n",
    "train, test = train_test_split(sampled_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = lightcoral> I think this dataset doesn't necessarily need stratification, since you would need to have several defined groups to compare. If you wanted to, you could divide the data into two strata based on gender if you wanted to compare it in more detail. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "**Exercise 1 C)** \n",
    "\n",
    "Standardize the numerical features: Note that you should now have two separate features that you've divided all the features into.\n",
    "\n",
    "- Describe how the k-NN model would make predictions about whether or not a patient has a CVD when the features are not standardized, and explain the reasons behind.\n",
    "\n",
    "\n",
    "*Note: Some good information about preprocessing and how to use it for train and test data can be found https://scikit-learn.org/stable/modules/preprocessing.html*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - Standardization\n",
    "\n",
    "#First let's separate the numerical features. \n",
    "numerical_features = ['age', 'height', 'weight', 'ap_hi', 'ap_lo']\n",
    "\n",
    "#Now that we've separated them, let's standardize them. First, let's make two copies.\n",
    "\n",
    "train_stand = train.copy()\n",
    "test_stand = test.copy()\n",
    "\n",
    "#then we can perform the standardization on numerical features on both the training and test sets\n",
    "\n",
    "for x in numerical_features:\n",
    "    scale = StandardScaler().fit(train_stand[[x]])\n",
    "    \n",
    "    train_stand[x] = scale.transform(train_stand[[x]])\n",
    "    \n",
    "    test_stand[x] = scale.transform(test_stand[[x]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = lightcoral> When the features are not standardized, the model would make incorrect predictions due to larger values dominating the distance calculation. The standardization would align better on a smaller range fixing this problem.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muqCazPIoDw4"
   },
   "source": [
    "-------\n",
    "\n",
    "It's time for us to train the model!\n",
    "\n",
    "**Exercise 1 D)**\n",
    "\n",
    "Train a k-NN model with $k=3$. Print out the confusion matrix and use it to compute the accuracy, the precision and the recall.\n",
    "- What does each cell in the confusion matrix represents in the context of our dataset?\n",
    "- How does the model perform with the different classes? Where do you think the differences come from? Interpret the performance metrics you just computed.\n",
    "- With our dataset, why should you be a little more cautious when interpreting the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NMR7Y2s6oDw4",
    "outputId": "33bd42f3-a25c-47de-cb12-908d698d08af"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6866666666666666, Precision: 0.3625, Recall: 0.4027777777777778\n"
     ]
    }
   ],
   "source": [
    "### Code - the kNN classifier\n",
    "\n",
    "#Let's create a KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "\n",
    "#Let's modify our target variable to for knn. If this is not done, the code throws a warning.\n",
    "train_label = np.ravel(train[labels])\n",
    "test_label = np.ravel(test[labels])\n",
    "\n",
    "#Then, let's fit the classifier to the data\n",
    "knn.fit(train_stand[features],train_label)\n",
    "\n",
    "#I used https://www.w3schools.com/python/python_ml_confusion_matrix.asp guidance on Confusion Matrixes below\n",
    "#Before printing out the confusion matrix we need to predict.\n",
    "prediction = knn.predict(test_stand[features])\n",
    "\n",
    "#Now we can create the confusion matrix.\n",
    "c_matrix = metrics.confusion_matrix(test_label, prediction)\n",
    "\n",
    "#Let's display it using the metrics library\n",
    "c_matrix_display = metrics.ConfusionMatrixDisplay(confusion_matrix = c_matrix, display_labels = [False, True])\n",
    "c_matrix_display.plot()\n",
    "plt.show()\n",
    "\n",
    "#Then, we compute the accuracy, precision and recall.\n",
    "acc = metrics.accuracy_score(test[labels], prediction)\n",
    "\n",
    "precision = metrics.precision_score(test[labels], prediction)\n",
    "\n",
    "recall = metrics.recall_score(test[labels], prediction)\n",
    "\n",
    "print(f\"Accuracy: {acc}, Precision: {precision}, Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = lightcoral> The top left cell is the false negative cell. In our context, it means that there is both no cardiovasc. disease nor the model predicted one. The top right cell is the false positive, in this context there is no cardiovascular disease but our model predicted one. The bottom left is the true negative, in this context there is a cardiovascular disease but our model didn't predict it. The last one, bottom right cell, is the true positive. In this context, there is a cardiovascular disease and our model also predicted it.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = lightcoral> The model was accurate at predicting the false negative. It didn't predict the disease well at all, since it missed most of them. I think our k might have an effect on these results. Trying out different K's might produce better and more precise results. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = lightcoral> In our dataset, cardiovascular disease is not very common, so if our model just guesses constantly no, it is most of the time right. As we could see, it was somewhat accurate, but not precise. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "You randomly divided the data into two sets, one for training the k-NN model and the other for evaluating its performance. However, randomness is not the thing we really need, and in fact, it's not something we even desire. Instead, what we do want is to keep track of each step we're making and exporing. This said, the *reproducibility* of the results is extremely important in research. To achieve this, we can utilize <font color = lightcoral> a random seed</font>, with which we can re-run the codes and get the exact same results than before.\n",
    "\n",
    "For example, we can use a fixed seed when we're shuffling the data before splitting it into training and test sets. This ensures that when we're re-runing the code, we obtain exactly the same partitions of the data in each split.\n",
    "\n",
    "**Exercise 1 E)**\n",
    "\n",
    "Initialize 1000 random seeds and continue with the k-NN model ($k=3$): Perform 1000 different train-test splits using these seeds and store the accuracies from each split. Plot the accuracies in a histogram, and discuss your results.\n",
    "\n",
    "*Tip: You can add the accuracy from the previous exercise in the plot by drawing a vertical line with the function `matplotlib.axes.Axes.axvline(<accuracy>)` if you want!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - Initialization of the 1000 fixed seeds\n",
    "\n",
    "#Let's initialize 1000 random seeds.\n",
    "seeds = np.random.randint(1, 1000, size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - 1000 different train-test-splits\n",
    "\n",
    "#Let's perform 1000 different train-test splits with our random seeds.\n",
    "\n",
    "#We'll store the accuracies in an array.\n",
    "accs = []\n",
    "\n",
    "for seed in seeds:\n",
    "    \n",
    "    #The 80/20 split\n",
    "    train, test = train_test_split(sampled_data, test_size=0.2, random_state = seed)\n",
    "    \n",
    "    #Let's modify our target variable to for knn. If this is not done, the code throws a warning.\n",
    "    train_label = np.ravel(train[labels])\n",
    "    test_label = np.ravel(test[labels])\n",
    "    \n",
    "    #Initializing model\n",
    "    knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "    \n",
    "    #Fitting our data\n",
    "    knn.fit(train[features], train_label)\n",
    "    \n",
    "    #Calculating accuracy\n",
    "    acc = knn.score(test[features],test_label)\n",
    "    \n",
    "    #Recording our accuracy\n",
    "    accs.append(acc)\n",
    "    \n",
    "#Plotting our accuracies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(accs, bins=30)\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = lightcoral> The first time our accuracy was about 0.71. From these results we can see, that the accuracies seem to form somewhat of a normality distribution -like histogram with 0.71 being somewhat in the center. We can conclude that with k=3 our model 71% accurate most of the time.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "One really common evaluation metric is <font color = lightcoral>the area under the receiver operating characteristic (AUROC, AUC or AUC-ROC)</font>.  It provides a measure of a model's ability to distinguish between classes, especially in binary classification tasks between the <i>positive class</i> and the <i>negative class</i>. (There sure are multiclass and multilabel cases too, but they are out of scope here.) \n",
    "\n",
    "In our case, individuals who have a CVD form the positive class. As the name of the measure suggests, it combines two keys aspects to interpret a model's performance: ROC curves are about the trade-off between the true positive rate and the false positive rate, the former representing the model's ability to correctly identify individuals with a CVD (true positives) and the latter measuring the model's tendency to incorrectly classify individuals without a CVD as if they have the disease (false positives). Thus, the area beneath the curve is simply the AUROC, a single numerical value, that summarizes the overall performance.\n",
    "\n",
    "**Exercise 1 F)** \n",
    "\n",
    "Evaluate the performance of the trained k-NN model by computing the AUROC and plotting the related curve. Draw also the line for random guesses.\n",
    "\n",
    "- How well does the k-NN model perform in distinguishing between healthy individuals and those with a CVD?\n",
    "\n",
    "*Tip: You should not use the predicted labels in this exercise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - ROC curve and AUROC\n",
    "\n",
    "#I'll reuse some code from earlier to save some time here. \n",
    "#Let's create a KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "\n",
    "#Let's modify our target variable to for knn. If this is not done, the code throws a warning.\n",
    "train_label = np.ravel(train[labels])\n",
    "test_label = np.ravel(test[labels])\n",
    "\n",
    "#Then, let's fit the classifier to the data\n",
    "knn.fit(train_stand[features],train_label)\n",
    "\n",
    "#Now let's get predicted probabilities: (I used https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/ for help here)\n",
    "prob = knn.predict_proba(test_stand[features])[:,1]\n",
    "\n",
    "#Roc curve\n",
    "fpr, tpr, thresh = metrics.roc_curve(test_label, prob, pos_label=1)\n",
    "\n",
    "#AUC\n",
    "auroc = metrics.auc(fpr, tpr)\n",
    "print(auroc)\n",
    "\n",
    "#Let's plot the ROC curve \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(fpr, tpr, color='blue')\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = lightcoral> This kNN model is not very good at distinguishing between healthy inviduals and those with CVD. You could as well as guess and get about the same result.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQfehqAioDw4"
   },
   "source": [
    "__________\n",
    "## <font color = royalblue> 2. Classification accuracy using leave-one-out cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPZx-6JLoDw5"
   },
   "source": [
    "While the train-test split may provide us with an unbiased estimate of the performance, we only evaluate the model once. Especially when dealing with small datasets, a test set itself will be very small. How can we be sure that the evaluation is accurate with this small test set and not just a good (or bad) luck? And what if we'd like to compare two models and the other seems to be better -- how can we be sure that it's not just a coincidence?\n",
    "\n",
    "Well, there's a great help available and it's called <font color = royalblue>cross-validation</font>. With its help, we can split the dataset into multiple different training and test sets, which allows us to evaluate models across various data partitions. This time, we'll take a closer look at the <font color = royalblue>leave-one-out cross-validation</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "\n",
    "Let's keep the focus on detecting the CVD, so once again we utilize the k-NN model (with $k=3$) to predict the precense of the disease. Now, apply leave-one-out cross-validation to assess whether the k-NN model is suitable for addressing the problem. You may use the entire dataset on this task.\n",
    "\n",
    "- What can you say about the accuracy compared to the previous task?\n",
    "- What do you think: Does the k-NN model work for the problem in hand? Explain your answer.\n",
    "\n",
    "*Tip: This sure can be done manually, but `cross_val_score` is quite a handy function too.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znHjVwKDoDw5",
    "outputId": "3b73cc4a-eef7-4e9e-be17-19d79df71033"
   },
   "outputs": [],
   "source": [
    "### Code - Leave-one-out cross-validation\n",
    "\n",
    "#Let's create a KNN classifier. I used https://machinelearningmastery.com/loocv-for-evaluating-machine-learning-algorithms/ for help in this part.\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "\n",
    "#Let's modify our target variable to for knn. If this is not done, the code throws a warning.\n",
    "label = np.ravel(sampled_data[labels])\n",
    "\n",
    "#Let's use the cross_val_score function here. The least populated class here has 457 members so let's perform on that many subsets.\n",
    "accs = cross_val_score(knn, sampled_data[features], label, cv=457)\n",
    "\n",
    "#Let's then calculate the mean from the accs\n",
    "mean = np.mean(accs)\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = royalblue> The accuracy seems to be quite equal to the 0.71 we had earlier. I think our k-NN model was quite bad at identifying people with CVD so I'd use some other model for this.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88BjCQL6oDw5"
   },
   "source": [
    "____________\n",
    "## <font color = forestgreen> 3. Model selection with leave-one-out cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8fcES_LoDw5"
   },
   "source": [
    "So far, we've trained one model at a time and I've given the value of k for you. Accuracy is what it is (no spoilers here), but could we still do a little better? Let's explore that possibility through a process known as <font color=green>hyperparameter tuning</font>. The cross-validation is especially important tool for this task. Note here, that model selection and model evaluation (or assessment) are two different things: We use model selection to estimate the performance of various models to identify the model which is most likely to provide the \"best\" predictive performance for the task. And when we have found this most suitable model, we *assess* its perfomance and generalisation power on unseen data.\n",
    "\n",
    "This time, we're going to train multiple models, let's say 30, and our goal is to select the best K-Nearest Neighbors model from this set. Most models come with various hyperparameters that require careful selection, and the k-NN model is no exception. Although we're talking about the number of neighbors here, it's important to note that k-NN also has several other hyperparameters. However, for the sake of simplicity, this time we'll focus solely on fine-tuning the number of nearest neighbors, that is, the value of k. \n",
    "\n",
    "Let's focus on the model selection part here for the sake of comprehending the cross-validation itself. We'll get later on the whole pipeline, which also includes model assessment.\n",
    "\n",
    "**Exercise 3**\n",
    "\n",
    "Find the optimal k value from a set of $k=1...30$ using leave-one-out cross-validation. Plot the accuracies vs. the k values. Again, you may use the entire dataset.\n",
    "\n",
    "- Which value of k produces the best accuracy when using leave-one-out cross-validation? Compare the result to the previous model with $k=3$.\n",
    "- If the number of k is still increased, what is the limit that the accuracy approaches? Why?\n",
    "- Discuss the impact of choosing a very small or very large number of neighbors on the k-NN model's ability to distinguish between the healthy individuals and the ones with CVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJ7570_ZoDw6",
    "outputId": "91caaf74-2636-46f7-a4e3-4e5ba98e4c4e"
   },
   "outputs": [],
   "source": [
    "### Code - Select best model\n",
    "\n",
    "#Let's modify our target variable to for knn. If this is not done, the code throws a warning.\n",
    "label = np.ravel(sampled_data[labels])\n",
    "\n",
    "#Let's establish the range for our k values. I used https://machinelearningmastery.com/how-to-configure-k-fold-cross-validation/ for help here.\n",
    "k_values = range(1,31)\n",
    "\n",
    "#Now let's perform LOOCV for every k value and record their accuracies.\n",
    "accs = []\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, sampled_data[features], label, cv=5) #As we know from last task, the least populated class has 457 members but let's use a lower value so the runtime wont be abysmal.\n",
    "    accs.append(np.mean(scores))\n",
    "\n",
    "#Let's select the best model by finding the largest accuracy.\n",
    "best_model = k_values[np.argmax(accs)]\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - Plot the accuracies vs. the values for k\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, accs, marker='o', linestyle='-')\n",
    "plt.title('Accuracy vs. Number of Neighbors (k)')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.xticks(k_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green> k=29 gives us the best result. Its accuracy is over 0.76 compared to the 0.72 of k=3. Should the k be further increased, the accuracy seems to stay somewhere within 0.76. The model already takes into account all of the relevant information and further increasing the k makes it more biased and causes underfitting. If the k is too small, the model is too sensitive for noise in the data which lowers its accuracy significantly. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9r2v1LEDoDw6"
   },
   "source": [
    "________________\n",
    "## <font color = red>  4. Training and testing on the same dataset </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6B6L5HWoDw6"
   },
   "source": [
    "<i>Note that this should never be done outside of this exercise! You have been warned.</i>\n",
    "\n",
    "Oh, but what if we just trained a model using the *entire* dataset? Wouldn't we like to use as much data as possible to discover the underlying patterns in the data so why not to use all of it?\n",
    "\n",
    "**Exercise 4**\n",
    "\n",
    "This is quite straightforward: Train 30 k-NN models ($k = 1...30$ ) using the whole dataset and evaluate the trained models using, again, the whole dataset. Create a plot that displays the accuracies against the corresponding k values. Include the accuracy values from the previous task in the same figure.\n",
    "\n",
    "- What's the optimal value for k now and why's that? How would you interpret the reliability of the predictions?\n",
    "- Explain why you should never use the same dataset for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExWjmjQ5oDw6",
    "outputId": "f7bd8eac-a636-400b-a8a4-9ca269129628"
   },
   "outputs": [],
   "source": [
    "### Code - Train with whole data\n",
    "#Let's modify our target variable to for knn. If this is not done, the code throws a warning.\n",
    "label2 = np.ravel(cardio_data[labels])\n",
    "\n",
    "#Let's establish the range for our k values. I used https://machinelearningmastery.com/how-to-configure-k-fold-cross-validation/ for help here.\n",
    "k_values2 = range(1,31)\n",
    "\n",
    "#Now let's perform LOOCV for every k value and record their accuracies.\n",
    "accs2 = []\n",
    "for k in k_values2:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores2 = cross_val_score(knn, cardio_data[features], label2, cv=5) #As we know from last task, the least populated class has 457 members\n",
    "    accs2.append(np.mean(scores2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, accs, color='blue', marker='o', linestyle='-')\n",
    "plt.plot(k_values2, accs2, color='red', marker='x', linestyle='--')\n",
    "plt.title('Accuracy vs. Number of Neighbors (k)')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.xticks(k_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = red> There seems to be a mistake in the plotting or I've understood the assignment from in some form. By using the entire dataset the model is now more accurate. But it only works for this data since if you use the entire dataset for training, the model will overfit and performs poorly if some new unseen data is given to it. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________\n",
    "\n",
    "## <font color = darkorange> 5. Comparison of ridge regression and k-NN regression </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous exercises were about classification. Now, we are ready to see another kind of supervised learning - regression - as we are changing our main goal from predicting discrete classes (healty/sick) to estimating continuous values. The following exercises are going to involve utilizing two different regression models, <font color = darkorange>Ridge Regression</font> and <font color = darkorange>K-Nearest Neighbors (k-NN) Regression</font>, and our goal is to evaluate and compare the performances of these two models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the dataset to make the following exercises more intuitive. The new dataset is about brushtail possums and it includes variables such as\n",
    "\n",
    "- <b>sex</b>: Gender, either male (0) or female (1)\n",
    "- <b>age</b>: Age in years\n",
    "- <b>len_head</b>: Head length in cm\n",
    "- <b>width_skull</b>: Skull width in mm\n",
    "- <b>len_total</b>: Total length in cm\n",
    "- <b>len_tail</b>: Tail length in cm\n",
    "- <b>len_foot</b>: Foot length \n",
    "- <b>len_earconch</b>: Ear conch length \n",
    "- <b>width_eye</b>: Distance from medial canthus to lateral canthus of right eye, i.e., eye width\n",
    "- <b>chest</b>: Chest grit in cm\n",
    "- <b>belly</b>: Belly grit in cm\n",
    "\n",
    "In this case, our target variable will be *the age of the possum*. The data for this exercise has been modified from the original source.\n",
    "\n",
    "There's the code chunk for loading data provided again. <font color = red>Again, the data file should be located in the same directory as this notebook file!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading code provided\n",
    "# ------------------------------------------------------\n",
    "# The data file should be at the same location than the \n",
    "# exercise file to make sure the following lines work!\n",
    "# Otherwise, fix the path.\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# Data path\n",
    "data_path = 'ex2_possum_data.csv'\n",
    "\n",
    "# Load the data \n",
    "possum_data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "Regression allows us to examine <font color = darkorange>relationships between two or more variables</font>. This relationship is represented by an *equation*, which itself represents how a change in one variable affects another on average. For example, we could examine how a change in possum's total length affects, on average, its estimated age.\n",
    "\n",
    "We start by examing those relationships between the variables in the given dataset.\n",
    "\n",
    "\n",
    "**Exercise 5 A)**\n",
    "\n",
    "Plot pairwise relationships between the age variable and the others where you color the samples based on the sex variable. \n",
    "\n",
    "- Which body dimensions seem to be most correlated with age? And are there any variables that seem to have no correlation with it?\n",
    "- Are there any differences in the correlations between males and females?\n",
    "\n",
    "*Tip: `seaborn.pairplot()` is handy with the parameters `(x,y)_vars` and `hue`. You actually can fit a linear model to draw a regression line with the parameter `kind` set to `\"reg\"`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - Pairplot\n",
    "\n",
    "#Let's plot pairwise relationships between age variable. I used https://www.geeksforgeeks.org/python-seaborn-pairplot-method/ as help here.\n",
    "sns.pairplot(possum_data, hue='sex', vars=['age', 'len_head', 'width_skull', 'len_total', 'len_tail', 'len_foot', 'len_earconch', 'width_eye', 'chest', 'belly'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = darkorange> Head length, tail length, total length, eye width, chest and belly seem to steadily grow and shrink with age. Males have generally larger chests, bellies, eye widths, feet and tails. Females have slightly larger heads at some point in their lives and in some aspects might outgrow males."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "Before the regression analysis itself, let's check that our dataset is in a proper format. We'll also perform the train-test split as we're going to first tune the hyperparameters for each model using the training set and test the overall performance of the chosen models using the test set.\n",
    "\n",
    "**Exercise 5 B)**\n",
    "\n",
    "Do you need to prepare the data a little? Explain your decision. Perform the train-test (80/20) split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - Data preparation\n",
    "\n",
    "#Let's check for missing values.\n",
    "print(possum_data.isnull().sum())\n",
    "\n",
    "#Let's collect features and separate the target variable, age. First let's separate them.\n",
    "\n",
    "labels = ['age']\n",
    "\n",
    "features = ['sex', 'len_head', 'width_skull', 'len_total', 'len_tail', 'len_foot', 'len_earconch', 'width_eye', 'chest', 'belly']\n",
    "\n",
    "#Let's then split the data into training and test set.\n",
    "train, test = train_test_split(possum_data, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = darkorange>\\<Write your answer here\\></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "Regarding the k-NN, we have already get familiar with the optimization of the k value. The idea behind the k-NN is the same as previously in classification, the output isn't a class anymore but a continuous value. So, for now, we can stick to the optimal k value. However, for Ridge Regression, we'll focus on the hyperparameter called $\\lambda$ (read as 'lambda'), the regularization term (or penalty term or L2 penalty, how ever we'd like to call it), and try to find its optimal value for this task. After the model selection for both regression is performed, we compare the chosen models using a metric called <font color = darkorange>mean absolute error (MAE)</font>.\n",
    "\n",
    "**Exercise 5 C)**\n",
    "\n",
    "Train multiple ridge regression models and k-NN regression models. For hyperparameters, use $\\lambda=2^{-10}...2^{10}$ and $k=1...30$. Once again, use leave-one-out cross validation. Remember to use only the training dataset for model selection. Plot the optimal k values and lambdas versus corresponding MAEs (two different plots).\n",
    "\n",
    "- Introduce the optimal hyperparameters for each regression model.\n",
    "- How do you interpret the MAE in our case when the target variable is age?\n",
    "\n",
    "*Note: In a `sklearn.linear_model.Ridge` class, lambda is called as \"alpha\" so don't get confused.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - CV for Ridge regression and k-NN regression\n",
    "\n",
    "# I used https://machinelearningmastery.com/ridge-regression-with-python/ and https://www.geeksforgeeks.org/how-to-calculate-mean-absolute-error-in-python/ for help here.\n",
    "\n",
    "#Let's initialize variables for storing optimal values and MAEs\n",
    "opt_ks = []\n",
    "opt_lambdas = []\n",
    "mae_knn = []\n",
    "mae_ridge = []\n",
    "\n",
    "#Let's introduce our hyperparameters.\n",
    "k_values = range(1,31)\n",
    "lambda_values = [2**x for x in range(-10, 11)]\n",
    "\n",
    "#Let's modify our target variable to for knn. If this is not done, the code throws a warning.\n",
    "label = np.ravel(train[labels])\n",
    "\n",
    "#LOOCV for Ridge Recession\n",
    "for l in lambda_values:\n",
    "    ridge = Ridge(alpha=l)\n",
    "    mridge = -cross_val_score(ridge, train[features], label, cv=5, scoring='neg_mean_absolute_error').mean()\n",
    "    mae_ridge.append(mridge)\n",
    "    \n",
    "#LOOCV for kNN recession\n",
    "for k in k_values:\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    mknm = -cross_val_score(knn, train[features], label, cv=5, scoring='neg_mean_absolute_error').mean()\n",
    "    mae_knn.append(mknm)\n",
    "\n",
    "#Let's find best lambda value for Ridge\n",
    "best_lambda = lambda_values[np.argmin(mae_ridge)]\n",
    "opt_lambdas.append(best_lambda)\n",
    "    \n",
    "#Let's find best k value for k-NN\n",
    "best_k = k_values[np.argmin(mae_knn)]\n",
    "opt_ks.append(best_k)\n",
    "\n",
    "#Let's create a plot for the kNN.\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(k_values, mae_knn, marker='o', linestyle='-')\n",
    "plt.xlabel('k values')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Optimal k values vs. Mean Absolute Error (k-NN)')\n",
    "plt.axvline(x=best_k, color='red', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "#Let's the create another plot for Ridge\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(lambda_values, mae_ridge, marker='o', linestyle='-')\n",
    "plt.xlabel('Lambda values')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Optimal Lambda values vs. Mean Absolute Error (Ridge Regression)')\n",
    "plt.axvline(x=best_lambda, color='red', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Our best k value according to MAE is {opt_ks} and best lambda value is {opt_lambdas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = darkorange> According to MAE, our most optimal k value for k-NN recession is k=14 and most optimal lambda value for Ridge Recession is lambda = 128. The MAE in this dataset represents the average by which predicted ages deviate from the actual ages.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "The cross-validation was employed for model selection and at this point, we have the optimal hyperparameter settings for each model. Let's finally assess both models using the test set. To continue from this, we first fit the chosen models using the entire training, ensuring that the models are trained with the maximum available data. \n",
    "\n",
    "**Exercise 5 D)**\n",
    "\n",
    "Fit the chosen models with the whole training set. Evaluate the models using the test set and describe the results.\n",
    "\n",
    "- How well did the models perform in estimating the possums' ages?\n",
    "- So, what's the pitfall in here if you had compared the overall performance of the models based on the cross-validation MAEs and not the MAEs for the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code - Evaluating the selected models\n",
    "\n",
    "#We know know the optimal k and lambdas values and can now estimate the possum ages.\n",
    "\n",
    "#Let's also transform our test labels. If this is not done it causes a warning.\n",
    "test_label = np.ravel(test[labels])\n",
    "\n",
    "#Let's first utilize Ridge Recession. Start with initializing it and then fit our training data.\n",
    "ridge = Ridge(alpha=best_lambda)\n",
    "ridge.fit(train[features], label)\n",
    "\n",
    "#Then, we'll make some predictions and evaluate using the MAE.\n",
    "ridge_pred = ridge.predict(test[features])\n",
    "mae_ridge = metrics.mean_absolute_error(test_label, ridge_pred)\n",
    "\n",
    "#Then, let's utilize kNN recession. Start with initializing it and then fit our training data.\n",
    "knn = KNeighborsRegressor(n_neighbors = best_k)\n",
    "knn.fit(train[features], label)\n",
    "\n",
    "#Then, we'll make some predictions and evaluate using the MAE.\n",
    "knn_pred = knn.predict(test[features])\n",
    "mae_knn = metrics.mean_absolute_error(test_label, knn_pred)\n",
    "\n",
    "#Let's print our findings\n",
    "print(f\"Fitting our models with training data shows us that kNN regression gave us a MAE value of {mae_knn:.3f} and Ridge Recession gave us a MAE value of {mae_ridge:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = darkorange> The Rdige model performed slightly better than the kNN model. If we hadn't utilized the test set, the model would have become too accustomed to the training data and some overfitting would have happened. The model would have produced some incorrect results in datasets it has not yet seen if it had some good luck in predicting learning patterns only found in the training dataset. </font>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DADK2020_ex3_Valtteri_Nieminen.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "180px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
